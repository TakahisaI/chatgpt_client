use super::Message;
use serde::Deserialize;

/// Represents the response from the ChatGPT API.
///
/// This struct contains information about the generated message, the choice made by the model,
/// the usage of tokens, and other metadata.
#[derive(Debug, Deserialize)]
pub struct Response {
    /// The unique identifier of the response.
    pub id: String,
    /// The object type, typically "chat.completion".
    pub object: String,
    /// The timestamp of when the response was created.
    pub created: u64,
    /// The list of choices generated by the model.
    pub choices: Vec<Choice>,
    /// Information about the token usage in the response.
    pub usage: TokenUsage,
}

/// Represents the token usage of the ChatGPT API response.
///
/// This struct provides information about the number of tokens used in the prompt,
/// the completion, and the total tokens used.
#[derive(Debug, Deserialize)]
pub struct TokenUsage {
    /// The number of tokens used in the prompt.
    pub prompt_tokens: u32,
    /// The number of tokens used in the completion.
    pub completion_tokens: u32,
    /// The total number of tokens used in the response.
    pub total_tokens: u32,
}

/// Represents a choice in the ChatGPT API response.
///
/// A choice is a generated message by the model, typically selected based on the highest
/// probability or other criteria such as temperature and top-p settings.
#[derive(Debug, Deserialize)]
pub struct Choice {
    /// The index of the choice in the response.
    pub index: u32,
    /// The message generated by the model, including its content and role.
    pub message: Message<String>,
    /// The reason for finishing the generation, such as "stop" (reached stop sequence),
    /// "length" (reached max tokens), or "eos" (end of sentence).
    pub finish_reason: String,
}
